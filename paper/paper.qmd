---
title: "Predictions of Occupied Beds at Faculties Providing Daily Shelter And/Or Overnight Services Using Multivariate Linear Regression Implies That Modelling This Situation Using Other Models Would Be Better"
author: 
  - Kevn Shao
thanks: "Code and data are available at: https://github.com/KevinShao1357/Toronto_Daily_Shelter_And_Overnight_Service_Modelling.git."
date: today
date-format: long
abstract: "Recently, the Canadian government expresses interest in continuous investment in government faculties providing daily shelter and/or overnight services. With the aim to evaluate the validity of predicting the occupied beds at a related faculties at a day, which can reflect the demand of such services at such locations, a multivariate linear regression model is constructed, with the city to Downtown Toronto, and the gender and age set to Mixed Adults. Five predictor variables are used, including two numerical variables, occupancy rate and capacity, and three categorical variables, including classification, service types, and program types. The final results showed likely violations of linearity assumptions such as homoscedasticity and normality of errors, and suggests that we may need to consider either proceed deeper in the linear regression model by adding nonlinear terms for predictor variables, identifying and deleting influential points, and/or consider the weighted-least squares (WLS) method, or consider switching to other more-flexible model types."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
geometry: margin=0.75in
header-includes:
  - \clearpage
---

```{r}
#| include: false
#| warning: false
#| message: false
#| echo: false
#| 
# Download the packages if necessary. If packages are already 
# downloaded, comment out the following lines.
install.packages("tidyverse")
install.packages("opendatatoronto")
install.packages("janitor")
install.packages("dplyr")
install.packages("readr")
install.packages("knitr")
install.packages("kableExtra")
install.packages("arrow")
install.packages("ggplot2")
install.packages("MASS")
install.packages("here")

# Load the packages
library(tidyverse)
library(opendatatoronto)
library(janitor)
library(dplyr)
library(readr)
library(knitr)
library(kableExtra)
library(arrow)
library(ggplot2)
library(MASS)
library(here)

# Read and save the cleaned dataset
cleaned_data2 <- read_parquet(here::here("data/02-analysis_data/analysis_data.parquet"))
```


# Introduction

Canada provides shelters for people in need, such as refugees, the homeless, and people experiencing transitions in living spaces, and the country is continuing to build new shelters for such residents. On April 16, 2024, the Canadian government announced a funding of over 250 million dollars over two years to address homelessness and encampment all over the country. In 2022, Canada announced a target to eliminate homelessness by 2030. Canada Mortgage and Housing Corporation also allocated a total of 420 million dollars to account for homelessness (@Shelter). This implicates that the Canadian government will continuously invest in improving current shelters and building new shelters around the country. In 2024, Canada's Minister of Immigration, Refugees, and Citizenship Marc Miller also announced that the country would accept 27,000 new refguees this year, and this trend is likely to continue in the next years (@Refugee). We can conclude that more shelters are likely to be built in Canada in the future, and the same trend is also valid for the GTA(Greater Toronto Area). 

To find a way to maximize efficiency of the investment on shelters, I want to evaluate the effect of predicting then number of beds (which reflects the necessary size of shelters need be built, expanded, or renovated) by constructing a multivariate linear regression model based on variables such as specific types of shelters and their services, as well as occupancy rates and averaged number of occupants per day. We will limit our scope of prediction to the GTA(Greater Toronto Area), since it is the largest city in Canada, and is also the scope of our chosen dataset. We also set the city of where the shelter is located to Downtown Toronto, and also set the classification of shelters based on gender and age to mixed adult, which are all the ones of their category with the highest frequency. As mentioned in the previous paragraph, we also chose our response variable to be number of beds, a choice between shelters grouped by rooms or shelters grouped by beds. The reasoning behind these choices are explained in the 'Measurement' of the data section (Section @sec-data).

After finishing the derivation of the final multivariate linear regression model, including the use of a box-cox transformation during the process, we can find that the model violates many linearity assumptions, such as linear relationship, normality of errors, and homoscedasticity (constant variance). The new model's residuals versus fitted and qq plots implicate that influential points should be identified and eliminated, and nonlinear transformations should be put on predictor variables. It can then be concluded that further steps can be made to the current multivariate linear regression model, but also more-flexible alternative model types, such as generalized additive models (GAMs), can be implemented to fulfill the same purpose with more precise final results. More detailed limitations and discussions to this result is discussed in detail in @sec-discussion.

The remainder of this paper is structured as follows: @sec-data details the data and measurement process of the chosen dataset; @sec-model covers the multivariate linear regression model used in this paper; @sec-results provides results obtain from the derived model; @sec-discussion includes discussions, such as implications and future steps, of our model, and how our results can in turn be furthermore improved; @sec-appendix is about analysis of observational data and surveys, and is the appendix of this paper.

# Data {#sec-data}

## Overview

The dataset "Daily Shelter & Overnight Service Occupancy & Capacity" (@Dataset) was sourced from the dataset with same name found in Open Data Toronto. The paper's in-depth analysis and prelimary data analysis are all completed using the statistical programming language R (@R). The book 'Telling Stories with Data' (@tellingstories) was also used to complete this paper.

Packages used to complete the paper and/or the scripts include ggplot2 (@ggplot2), tidyverse (@tidyverse), opendatatoronto (@opendatatoronto), janitor (@janitor), dplyr (@dplyr), readr (@readr), knitr (@knitr), kableExtra (@kableExtra), arrow (@arrow), testthat (@testthat), car (@car), here (@here), and MASS (@MASS).

The primary objective of this paper is to evaluate the validity of predicting the number of beds necessary for a shelter by constructing a multivariate linear regression model, as mentioned in the introduction section. The number of beds of a shelter can reflect the necessary size of a shelter, so the general investment amounts of renovating existing shelters and building new shelters can be estimated, which can maximize efficiency of Canadian government's investments in shelters.

@tbl-table1 provides a sample (the first ten rows) of the cleaned dataset.

```{r}
#| label: tbl-table1
#| fig-cap: "Sample Data of Cleaned Data of Daily Shelter and Overnight Service"
#| echo: false
#| warning: false
#| message: false

kable(cleaned_data2[1:10,]) %>%
  kable_styling(font_size = 10, full_width = FALSE)

```

## Measurement
	
Since the aim of this paper is only to evaluate the validity of predicting the number of beds necessary for a shelter by constructing a multivariate linear regression model, we want to simplify the dataset as much simple as possible. 

We first consider the classification of shelters based on gender and age, which is divided into adult men, adult women, mixed adult, youth and family. We find that mixed adult shelters takes the majority with the most observations, so we conclude that mixed adult shelters are most representative for checking the validity of the multivariate linear regression model for predicting number of beds for a shelter, so we filter out so that only mixed adult shelters are in the cleaned dataset.

Downtown Toronto has the most shelters in all the cities of GTA(Greater Toronto Area) in this dataset, so we also conclude that Downtown Toronto is the most representative of all the provided cities, so we then filter out the cleaned data so it only contains shelters in Downtown Toronto.

In this dataset, the shelters are either grouped by rooms or by beds. Since our aim is to analyze the validity of maximizing efficiency of investments of shelters using a multivariate linear regression model, considering number of beds will bring more accurate results. This is because each room can vary in size, so it is not a precise measure of the demand of shelters, which is ultimately based on the number of people, while it is more accurate to assume that one bed can accommodate one person. Therefore, we then filter out the cleaned data so it only contains shelters in Downtown Toronto.

In the original dataset, each observation of shelters vary by date. However, because we are discussing about investments in shelters, which is a long-term process, and our response variable is the number of beds necessary in a shelter, we want to get an average of the number of beds to maximize the efficiency of shelter investments by the Canadian government. Therefore, we ignore the date column, and just aim to construct a multivariate linear regression model considering all days from January 1 to December 1, which is the version of the dataset we chose to use, which is updated until December 2, 2024.

Then, we determine our predictor variables, made up of three categorical variables, which are classification, program area, and service type, and two numerical variables, which are capacity and occupancy rate. The rationale here is basically just ignoring the columns which only provide basic information, and then save all the remaining categorical variables (leaving the different types of shelters variables that are prepared to be involved in the multivariate linear regression model, after fixing the city, as well as general age and gender of the shelters) and numerical variables that are logical to predict the number of beds. Finally, we rename the variables to make them easier to understand, and we get our final cleaned dataset.

## Outcome variables (And Estimand)

The outcome variable, or response variable, for this multivariate linear regression model, corresponds to the variable 'count' in the cleaned data set, which in context of the dataset, is the number of occupied beds during this day of a specific location that provides daily shelter and/or overnight service. By averaging this variable's statistics over about a year, we may get a representative value of the number of beds necessary for a shelter with the given statistics of the predictor variables. Note that the variable 'count' should be less or equal to the current capacity of the corresponding shelter. Its summary statistics are presented in the following @tbl-table2.

```{r}
#| label: tbl-table2
#| tbl-cap: "Summary Statistics of Number of Beds Occupied During a Given Day"
#| echo: false
#| warning: false
#| message: false

knitr::kable(c(mean = mean(cleaned_data2$count), standard_deviation = sd(cleaned_data2$count), maximum = max(cleaned_data2$count), minimum = min(cleaned_data2$count)), align = "c", col.names = ("Beds Occupieds in a Given Day"))
```
From the above summary statistics presented in @tbl-table2, the mean of 'count' is around 50 to 51 beds occupied in a given day of a shelter, the standard deviation is around 34, and the maximum and minimum are 150 and 1 respectively. Here, we can see that 'count' has some outliers, and also has reasonable mean and standard deviation, and so 'count' is suitable for being the response variable of our multivariable linear regression model.

Since we are trying to constructing a multivariate linear regression model, our estimand is the response variable, which is the the number of occupied beds during a day (can be understood as the demand of beds) in a location that provides daily shelter or overnight service.

## Predictor variables

As mentioned in the previous measurement section, our predictor varibles are made up of three categorical variables and two numerical variables, and are already chosen, because they are the only ones that are appropriate and logical to predict the number of occupied beds at a given day for a shelter. The following is a more specific description for each of the predictor variables.

### Service Type

'Service Type' (dis a categorical predictor variable, representing the type of the overnight service being provided. Here, because we set the city to be Downtown Toronto, as well as age and gender to Mixed Adult, so only the following types exist:

A 'Shelter' is a facility that give people experiencing homelessness a temporary living space so that they can move into new housing. Shelters operate the entire year for 7 days a week and 24 hours a day (@Dataset). 

A 'Warming Centre' prevents people from experiencing extreme weather, giving people a shelter to stay in, and only opens with operation of 7 days a week and 24 hours a day during extreme weather alerts.

A '24-Hour Respite' gives people experiencing homelessness a resting place, also providing them with "meals and service referrals." It operates 7 days a week and 24 hours a day.

A 'Top Bunk Contingency Space' is not described in the dataset, but is also another type of shelter.

The following @tbl-table3 describes the number of each service type in the cleaned data. Note that 'Respite' refers to 24-Hour Respite, and 'Cont_Space' refers to Top Bunk Contigency Space.

```{r}
#| label: tbl-table3
#| tbl-cap: "Count for each service type"
#| echo: false
#| warning: false
#| message: false

knitr::kable(c(Shelter = sum(cleaned_data2$service_type == 'Shelter'), 
               Warming_Centre = sum(cleaned_data2$service_type == 'Warming Centre'),
               Respite = sum(cleaned_data2$service_type == '24-Hour Respite Site'),
               Cont_Space = sum(cleaned_data2$service_type == 'Top Bunk Contingency Space')),
             align = "c", col.names = ("Beds Occupieds in a Given Day"))
```
The above @tbl-table3 clearly represents that the four service types have very different frequencies. Shelters take the most frequency, of around 4000 observations, while 24-Hour Respites take up of around 2000 observations, about half of that of shelters, and other two have the least frequency, each of around 400 observations.

### Classifcation

The variable 'classification' corresponds to the original dataset's variable 'PROGRAM_MODEL', and are directed to either 'Emergency' or 'Transitional'. Basically, a location classified as 'Emergency' means that any people experiencing homelessness can come in without a referral, and vice versa. 

@tbl-table4 below is a table representing the counts of each of the two classifications of all the observation in the cleaned dataset.

```{r}
#| label: tbl-table4
#| tbl-cap: "Count for each classification"
#| echo: false
#| warning: false
#| message: false

knitr::kable(c(Emergency = sum(cleaned_data2$classification == 'Emergency'),
               Transitional = sum(cleaned_data2$classification == 'Transitional')),
             align = "c", col.names = ("Counts For Each Classification"))
```
From @tbl-table4, we can see that there are much more emergency locations (around 5400) than transitional locations (around 1400). There are around three times more emergency locations than transitional locations.

### Program Area

'Program Area' (denoted as 'program_area' in the cleaned dataset) "indicates whether the program is part of the base shelter and overnight services system, or is part of a temporary response program". For this cleaned dataset with the set limitations, it includes the following types.

A 'Base Program - Refugee' is a program that serves refugees and other similar groups of people, and also operates the whole year.

A 'Winter Program' is a program based on the additional spaces under winter service plans. This may also add additional spaces to existing programs. In the dataset description, it is denoted as 'Winter Response'.

A 'Base Shelter and Overnight Services System' are regular programs that are set to operate the whole year.

A 'Temporary Refugee Response' is similar to that of a 'Base Program - Refugee', but instead "create spaces in the overnight services systems" (@Dataset).

@tbl-table5 below represents the counts of observations that have each program area. Note that 'Refugee' refers to 'Base Program - Refugee', 'Base_Shelter' refers to 'Base Shelter and Overnight Services System', and 'Temp_Refugee' refers to 'Temporary Refugee Response'

```{r}
#| label: tbl-table5
#| tbl-cap: "Count for each program area"
#| echo: false
#| warning: false
#| message: false

knitr::kable(c(Refugee = sum(cleaned_data2$program_area == 'Base Program - Refugee'),
               Base_Shelter = sum(cleaned_data2$program_area == 'Base Shelter and Overnight Services System'),
               Temp_Refugee = sum(cleaned_data2$program_area == 'Temporary Refugee Response'),
               Winter_Programs = sum(cleaned_data2$program_area == 'Winter Programs')),
             align = "c", col.names = ("Counts For Each Classification"))
```
From @tbl-table5, we observe that locations with the program area 'Base Shelter and Overnight Services System' has most observations (around 4800). Locations with the program area 'Base Program - Refugee' has 1272 observations, around one-fourth of the previous one. The remaining two locations has much less observations.

### Capacity

The 'Capacity' (denoted 'capacity' in the cleaned dataset) is the maximum capacity of a location. The following @tbl-table6 gives the mean, standard deviation, maximum, and minimum of 'Capacity'.

```{r}
#| label: tbl-table6
#| tbl-cap: "Summary Statistics of Variable Capacity"
#| echo: false
#| warning: false
#| message: false

knitr::kable(c(mean = mean(cleaned_data2$capacity), standard_deviation = sd(cleaned_data2$capacity), maximum = max(cleaned_data2$capacity), minimum = min(cleaned_data2$capacity)), align = "c", col.names = ("The Capacity of a Location"))
```
From @tbl-table6, the mean of capacity of locations is around 51 people, with an acceptable standard deviation of around 34 people. The maximum and minimum of capacity are 150 and 1 people respectively, and these statistics are all acceptable for constructing a multivariable linear regression model.

### Occupancy Rate

The occupancy rate here (denoted as 'occupancy_rate' in the cleaned dataset) is basically the occupied number of beds divided by the capacity in number of beds. The occupancy rate is measured in percentage, from 0 to 100 percent.

The following @tbl-table7 represents the summary statistics of occupancy rate.

```{r}
#| label: tbl-table7
#| tbl-cap: "Summary Statistics of Occupancy Rate"
#| echo: false
#| warning: false
#| message: false

knitr::kable(c(mean = mean(cleaned_data2$occupancy_rate), standard_deviation = sd(cleaned_data2$occupancy_rate), maximum = max(cleaned_data2$occupancy_rate), minimum = min(cleaned_data2$occupancy_rate)), align = "c", col.names = ("Occupancy Rate in a Given Day"))
```
From @tbl-table7, we can conclude that occupancy rate of locations are all pretty much near 100, with a relatively low standard deviation of 5 percentage points, so most locations are relatively full, but we can still consider it as a predictor variable, but needs careful consideration when interpreting results.

# Model {#sec-model}

## Model Set-Up (Including Diagnostics and Checking)

Now we start to create the multivariate linear regression model, with the set predictor and response variables. Specifically, the predictor variables in this model are made up of three categorical variables and two numerical variables. The three categorical predictor variables are service type, program area, and classification. The two numerical predictor variables are capacity and occupancy rate. The response variable is count, which is the number of occupied beds during a day for a specific location. The detailed descriptions of the predictor and response variables can be found in the 'Outcome variables (And Estimand)' and 'Predictor Variables' sections in @sec-data.

We first created a basic multivariate linear regression model with the set predictor and response variables. However, we notice that by official descriptions by Open Data Toronto (@Dataset) that occupancy rate is equal to count divided by capacity. Therefore, there is likely a strong relationship between the two predictor variables occupancy rate and capacity, so we add an interaction variable between occupancy rate and capacity, and construct another multivariate linear regression model.

We now have to test the validity of the two models, evaluating which one is better. To do that, we create @tbl-table8 below, presenting a comparison between the p-value, r-squared, and standard error of the two models, with 'Original_Model' representing the first one and 'Interaction_Model' being the one with the added interaction terms.

```{r}
#| label: tbl-table8
#| tbl-cap: "Comparsion of Features of Original and Interacted Model"
#| echo: false
#| warning: false
#| message: false

# Create both the original model and the model with interaction terms
model_a <- lm(count ~ service_type + program_area + classification + capacity +
              occupancy_rate, data = cleaned_data2)
model_b <- lm(count ~ service_type + program_area + classification + capacity +
              occupancy_rate + occupancy_rate:capacity, data = cleaned_data2)

# Summarize the model using R functions
model_sum1 <- summary(model_a)
model_sum2 <- summary(model_b)

# Calculate p-value for each using R functions
p_value1 <- pf(
  model_sum1$fstatistic[1],  
  model_sum1$fstatistic[2],  
  model_sum1$fstatistic[3],  
  lower.tail = FALSE  
)

p_value2 <- pf(
  model_sum2$fstatistic[1],  
  model_sum2$fstatistic[2],  
  model_sum2$fstatistic[3],  
  lower.tail = FALSE  
)

# Extract the r-squared and residual standard errors for each
r_squared1 <- model_sum1$r.squared
residual_std_error1 <- model_sum1$sigma
r_squared2 <- model_sum2$r.squared
residual_std_error2 <- model_sum2$sigma

# Create a data frame with three rows and two columns that contains the data
table_data <- data.frame(
  Label = c("P-Value", "R-Squared", "Residual Standard Error"),
  Original_Model = c(p_value1, r_squared1, residual_std_error1),
  Interaction_Model = c(p_value2, r_squared2, residual_std_error2)
)

# Make the data frame into the final displayed table
kable(
  table_data, 
  col.names = c("", "Original Model", "Interaction Model")
)
```

From the above @tbl-table8, both models have a p-value that is way less than the significance level of 0.05, and both have an r-squared that is extremely close to 1, meaning that a very high proportion of variance of the response variable can be explained by the predictor variables. This implicates that both multivariate linear regression models are highly valid models. However, the residual standard error of the second model with the interaction term is less than that of the first model without the interaction term, so we choose the second model with the interaction term.

Moreover, also note that since the occupancy rate of a shelter is equal to the number of people living in that shelter ("count") divided by the capacity of that shelter. Moreover, when the Toronto city government builds a shelter with a higher capacity, this typically means that they forecast the demand of shelters at this location, so the number of people living in that shelter on average is also typically higher. Therefore, the two variables should have a logical relationship, and so adding an interaction term between the two is also logical.

Now, the next task is to check if the new model with interaction terms satisfy all linearity assumptions. To finish that, we will first graph the residual versus fitted plot, which is the @fig-figure1 below.

```{r}
#| label: fig-figure1
#| tbl-cap: "Residual Versus Fitted Plot of New Model"
#| echo: false
#| warning: false
#| message: false

# Calculate residuals and fitted values respectively
res <- residuals(model_b)
fit <- fitted(model_b)

# Create a data frame with these residuals and fitted values
data_frame1 <- data.frame(
  residuals = res,
  fitted = fit
)

# Graph the residuals versus fitted plot
ggplot(data_frame1, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +  # Reference line at 0
  labs(x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()

```
Here, according to the residuals versus fitted plot presented in @fig-figure1, the linearity of the relationship assumption is satisfied because there is no systematic pattern, since points in this plot are scattered all over the place. Homoscedasticity, or constant variance, is violated, since we see a conical shape, reflecting a variable variance. Therefore, we log the response variable and then graph the qq plot again, as the below @fig-figure2.

```{r}
#| label: fig-figure2
#| tbl-cap: "Residual Versus Fitted Plot of New Model With Response Variable Logged"
#| echo: false
#| warning: false
#| message: false

# Create the new linear model accordingly
model_c <- lm(log(count) ~ service_type + program_area + classification + capacity +
              occupancy_rate + occupancy_rate:capacity, data = cleaned_data2)

# Calculate residuals and fitted values respectively
res3 <- residuals(model_c)
fit3 <- fitted(model_c)

# Create a data frame with these residuals and fitted values
data_frame2 <- data.frame(
  residuals = res3,
  fitted = fit3
)

# Graph the residuals versus fitted plot
ggplot(data_frame2, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +  # Reference line at 0
  labs(x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()

```
@fig-figure2 shows that now, the residual versus fitted plot shows that it both violates the linearity relationship assumption, since there is a clear pattern of the data points, and the homoscedasticity assumption, since the variance still varies, as seen in the graph. Therefore, the new model with logged response variable is even worse than the one that only added the interaction term, so we return the model that added the interaction terms. 

Our next step is to determine the specific transformation that should be done to the response variable using the box-cox transformation method. Before we do that, we want to check one more time whether to use to use the pre-logged model or the original model. We do that by comparing the qq plots between the two models. @fig-figure3 is the qq plot for the pre-logged model, and @fig-figure4 is the qq plot for the original model.

```{r}
#| label: fig-figure3
#| tbl-cap: "Theoretical Versus Sample Quantiles Plot for Pre-logged Model With Interaction Terms"
#| echo: false
#| warning: false
#| message: false

# Create dataframe for residuals
residuals_df2 <- data.frame(
  res = res,
  theoretical = qnorm(ppoints(length(res)))
)

# Construct the qq plot
ggplot(residuals_df2, aes(x = theoretical, y = res)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Q-Q Plot of Residuals",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")

```

```{r}
#| label: fig-figure4
#| tbl-cap: "Theoretical Versus Sample Quantiles Plot for Original Model"
#| echo: false
#| warning: false
#| message: false

# Calculate and save residuals for original model
res_pre <- residuals(model_a)

# Create dataframe for residuals
residuals <- data.frame(
  res = res_pre,
  theoretical = qnorm(ppoints(length(res_pre)))
)

# Construct the qq plot
ggplot(residuals, aes(x = theoretical, y = res)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Q-Q Plot of Residuals",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")

```

The two qq plots have barely any difference, and with the previous analysis of the residuals standard errors and p-values, we still eliminate the original model and choose the model with interaction terms but before we log the response variable. Now we proceed with the Box-Cox transformation, update the dataset and model, and then create the residuals versus fitted (@fig-figure5) and qq plot (@fig-figure6) for this new model.

```{r}
#| label: fig-figure5
#| tbl-cap: "Residuals Versus Fitted Plot For New Model (After Box-Cox Transformation)"
#| echo: false
#| warning: false
#| message: false

# Perform box-cox transformation 
boxcox_result <- boxcox(model_b, lambda = seq(-2, 2, by = 0.1), plotit = FALSE)

# Find the optimal lambda
optimal_lambda <- boxcox_result$x[which.max(boxcox_result$y)]

# Transform Y and refit the model
if (optimal_lambda == 0) {
  cleaned_data2$count_transformed <- log(cleaned_data2$count)
} else {
  cleaned_data2$count_transformed <- (cleaned_data2$count^optimal_lambda - 1) / optimal_lambda
}

# Refit the model with the transformed Y
new_model <- lm(count_transformed ~ service_type + program_area + classification + capacity +
                  occupancy_rate + capacity:occupancy_rate, data = cleaned_data2)

# Get the new model's residuals and fitted values, combine them into a dataframe
# and then create a residuals versus fitted plot
res_new <- residuals(new_model)
fit_new <- fitted(new_model)

data_new <- data.frame(
  residuals = res_new,
  fitted = fit_new
)

ggplot(data_new, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +  # Reference line at 0
  labs(title = "Residual Plot",
       x = "Fitted Values",
       y = "Residuals")
```

```{r}
#| label: fig-figure6
#| tbl-cap: "Theoretical Versus Sample Quantiles Plot For New Model (After Box-Cox Transformation)"
#| echo: false
#| warning: false
#| message: false

residuals_df_new <- data.frame(
  res = res_new,
  theoretical = qnorm(ppoints(length(res_new))
  )
)

ggplot(residuals_df_new, aes(sample = res_new)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals",
       x = "Theoretical Quantiles",
       y = "Sample Quantiles")
```

The qq plot of the new model after completing the box-cox transformation (@fig-figure6) has improved compared to the previous one. The end with the more negative theoretical quantiles values now only has gradual increasing deviations from the ideal line that the data points should follow, which is marked in red. However, a heavy tail still exists at the end with positive theoretical quantiles, suggesting possible outliers when we only look at the qq plot itself.

Now we observe the residuals versus fitted plot for the new model after the box-cox transformation (@fig-figure5). This plot data points, with the x-axis representing fitted values and the y-axis representing residuals, displays a few semi-circle patterns, which suggests even more severe violations of homoscedasticity, or constant variance. Therefore, I should consider adding polynomial terms at this point, but the method for deciding the specific polynomial terms is over the scope of my knowledge, so I just stop here and save the current model as our temporary final model. Also note that from this residuals versus fitted plot, we see that the new model also violates the linearity relationship assumption, but since it at least provides a possibility to add polynomial terms to our predictor variables, I am on the right track, so I still save this new model.

Note that this 'Model Setup' is only a brief summary of what I did in the model section, and the '05-model_data' file in the scripts section and the file in the models sections contains more details of the model derivation process. Please read the 'README' file for more detailed information of the structure of this Github repository.

# Results {#sec-results}

Our results are summarized in @tbl-table9.

```{r}
#| label: tbl-table9
#| echo: false
#| warning: false
#| message: false

# Extract coefficients and the intercept
coef <- coef(new_model)

# Create a data frame for the coefficients and intercept
coef_tbl <- data.frame(
  Coef = coef   
)

# Display the table using kable
kable(
  coef_tbl,
  col.names = c("Coefficient"),
  caption = "Coefficients and Intercept for the Linear Regression Model"
)
```

To this point, we can conclude that using multivariate linear regression models to predict the demand of shelters at a location, aimed to precisely control the amount of investment on shelters by Canadian government is a complicated approach, since it includes necessary further steps such as adding polynomial terms, checking influential points, and/or using weighted least squares (WLS) methods. Therefore, we must admit that switching to more flexible models such as generative additive models (GAM) may be a better choice to proceed furthermore.

# Discussion {#sec-discussion}

Although the derived results of the constructed multivariate linear regression model violates many linearity assumptions, including homoscedasticity, linear relationship, and normality of errors, and I stop since it is out of scope of my current knowledge, I can still discuss further steps and discussions of previous results.

At this point, please also note that it is also a challenge to predict the average demand of specific shelters (the response variable), since after we proceed with the box-cox transformations, we should not interpret the results by back-transforming again, so it would be hard for predicting the specific values of our response variable.

## Next Steps

### Polynomial Terms

As mentioned in the previous sections, from the residuals versus fitted plot of the newest model (@fig-figure5), to violate less of linearity assumptions such as linear relationship and homoscedasticity, what I should first do is to add nonlinear relationships to the predictor variables. Such approaches may include adding polynomial terms, such as capacity + (capacity)^2, logging our predictors, such as transforming capacity to log(capacity), and transforming our predictors using exponential terms, such as from capacity to exp(capacity). 

In my newest linear regression model, transforming my predictors to nonlinear ones is logical. This is mainly due to the corresponding residuals versus fitted plot, which shows a clear pattern between the residuals versus fitted values instead of scattered, pattern-less data points.

### Checking and Removing Influential Points

From the qq plot of the newest linear regression model after the box-cox transformation (@fig-figure6), we see a heavy tail on the end with positive theoretical quantiles. This may suggest possible outliers, which are data points that have extreme values on the response variable, and/or leverage points, which are data points that have extreme values on the predictor variables. Therefore, the best approach to consider that takes in consideration of both types of points is to determine and remove the influential points, which takes in the most extreme values of both by determining which observations have unusual large impacts on the fitted regression model itself. This determination and elimination process can be proceeded by either calculating DFBETA values or Cook's distance, both methods using reasonable set thresholds.

In this model derivation process, the step of identifying and eliminating influential points was not proceeded, since it should be done after adding nonlinear terms on predictor variables (and this step exceeded my scope of knowledge)

### Next Steps

Other steps that I could complete in the future include the following.

I can first consider weighted less squares (WLS) after completing the procedures described above to account for addressing heteroscedasticity. The new model may not entirely eliminate violations of homoscedasticity, or constant variance, since this approach is best to apply when variance increases or decreases systematically with fitted values, but it at least diminishes this violation.

After this, the final step for constructing the multivariate linear regression model would be using the AIC(Akaike Information Criterion)/BIC(Bayesian Information Criterion) to determine the best model of all numbers of predictors, ranging up to the current most-complex model. Note that our model is predictive, since we want to predict the demand of shelters ("count"), and there is a possibility of a larger scope in the future, so we will choose AIC in this case, but this is also part of further steps.

However, we must also consider to use an alternative model instead of multivariate linear regression model to address the relationship between the predictor and response variables. For example, higher-end models such as generalized additive models (GAMs), as well as machine-learning methods such as decision trees and random forests are all more-complicated ones that provide a more flexible approach to address the relationship.

## Further Steps After Completing The Above Possible Procedures

### Enlargening The Scope

Another way to possibility improve the constructed multivariate linear regression model is to enlarge the scope. In this linear regression model, the chosen city was set to be Downtown Toronto, which is only one part of Greater Toronto Area (GTA), and so is an even smaller part of Canada, which is the world's second largest country. To make the model more representative of Canada as a whole, and to make it more useful for predicting the needed number of beds for a location per day, we can enlargen the scope to the entire GTA, or possibly the entire province of Ontario, which may make results of the model more effective for predicting the amount of investment needed in these facilities, or to make good use of current investment budgets on such facilities providing daily shelter and overnight services.

Moreover, in this linear regression model, the gender and age that a specific location provides service to is also set to 'Mixed Adults'. To make the linear regression model even more effective and accurate, one can also enlarge this scope to contain all genders and ages by directly making it a categorical variable. One can also consider making independent linear regression models, or even make use of the more-complicated Bayesian model, for each of the following specific types of genders/ages of groups of people provided by locations, further increasing precision.

### Making The Date a Predictor Variable

In this derived multivariate linear regression model, date is not a predictor variable, and the model is based on a dataset that only considers data from January 1 to December 1, 2024. However, one could make it a continuous predictor variable to increase precision and/or accuracy. For example, if the Canadian government decide to invest in such facilities that provide daily shelter and/or overnight service on November 16, 2026, one could make use of the newly-derived function, and put the date as the date predictor variable, and get a rough prediction of how many beds would be occupied at that specific date and location for a specific facility, aiding them to effectively use investments to improve the entire shelter system.

## Further Discussions

Using the current multivariate linear regression model, one can get an estimate of occupied beds at a random day at a specific location of such a facility. By simply considering the amount of facilities providing daily shelter and overnight service, one can get an estimate of the total occupied beds at a random at a specific location of all such facilities in Downtown Toronto. After estimating the size needed for each bed, one can get the total area of facilities needed in Downtown Toronto, which gives the Canadian government a sense of which facilities needs to be renovated, and whether there are any new facilities needed to be built at locations due to shortage of space and/or old facilities that can be stopped using due to excessive space. Actually, if found useful after a time period, this approach can be used to the entire GTA(Greater Toronto Area), and can further expand to the entire Ontario, and later, the entire Canada in the future if proved to be efficient enough. 

\newpage

\appendix

# Appendix A: Analysis of Observational Data and Surveys {#sec-appendix}

Here is just some analysis of my constructed multivariate linear regression model, as well as some of my understandings of this provided dataset.

As I have been analyzing the cleaned data all the time, I am only being in touch with a subset of the Open Data Toronto that is about daily shelter and overnight service occupancy and capacity that I have chosen. Therefore, I may have stepped into the Simpson's Paradox, which occurs "when we estimate some relationship for subsets of our data, but a different relationship when we consider the entire dataset" (@tellingstories). The Simpson Paradox is an instance of the ecological fallacy, which basically occurs when we try to conclude some characteristics of an individual based on their group. This paradox is also certain to be possible to happen on my analysis, since I only chose the city Downtown Toronto, whereas the original dataset is about information of same facilities, but for the entire Greater Toronto Area (GTA). For example, if for the current data about the city Downtown Toronto, the residual plots show a clear conical shape, showing a violation of the linearity assumption homoscedasticity (constant variance), then I would naturally try to log the response variable to adhere more to this assumption. After this approach, I follow the other typical procedures to construct a final multivariate linear regression model. Then, at this moment, if I then generalize the model for all cities in Greater Toronto Area (GTA), an issue may occur if for certain other cities, no conical shape appears in other residual plots, then there may exist a possibility that for these cities, when I log the response variable, the homoscedasticity assumption is then unexpectingly violated. If the frequency of these cities are too high, the model I constructed for the city Downtown Toronto may not be valid as a whole.

A similar paradox Berkson's paradox may also occur similarly. Berkson's paradox occurs when "we estimate some relationship based on the dataset that we have, but because the dataset is so selected, the relationship is different in a more general dataset" (@tellingstories). Take the cleaned version of my chosen dataset again, the current multivariate linear regression model is only relatively valid, and is not valid to the highest extent, due to the possible violation of the linearity assumptions of homoscedasticity and/or normality of errors. However, this may be not be the case for the entire dataset which has the scope of the entire Greater Toronto Area (GTA), so the model between the same predictor and response variables may be more valid, less valid, entirely valid, or not valid at all, depending on the key characteristics of variables of other subsets based on the scope of other cities in GTA.

Let's also analyze some information about the data collection process of the same Open Data Toronto dataset named 'Daily Shelter & Overnight Service Occupancy & Capacity'. According to the official website page of the dataset on Open Data Toronto, the updated information about shelter and overnight service programs, including the program's operator, location, classification, occupancy, and capacity, which are all "administered by TSSS(Toronto Shelter and Support Services" (@Dataset). However, we cannot make sure that the administration of TSSS is certainly free of mistakes, so some parts of the dataset may also be done with convenience sampling, which is a type of non-probability sampling when researchers collect data which are easiest to accest from their perspective. If this hypothesis holds true, then even if the entire dataset used, the model can be still not representative of all the related facilities in the Greater Toronto Area (GTA).
\newpage

# References


